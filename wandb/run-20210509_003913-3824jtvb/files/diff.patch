diff --git a/__pycache__/options.cpython-37.pyc b/__pycache__/options.cpython-37.pyc
index 6c8e324..73d1304 100644
Binary files a/__pycache__/options.cpython-37.pyc and b/__pycache__/options.cpython-37.pyc differ
diff --git a/main.py b/main.py
index a9ab555..4f67b99 100644
--- a/main.py
+++ b/main.py
@@ -1,7 +1,9 @@
 import torch
+import torchvision.transforms as transforms
 from tqdm.auto import tqdm
 import wandb
 from wilds import get_dataset
+from wilds.common.data_loaders import get_train_loader, get_eval_loader
 
 from models import DeepDANN
 import options
@@ -10,7 +12,7 @@ import datetime
 import logging
 import os
 import time
-"""
+
 logger = logging.getLogger(__file__)
 logger.setLevel(logging.DEBUG)
 fmt = logging.Formatter("[%(levelname)s] %(asctime)s %(filename)s:%(lineno)d - %(message)s\n")
@@ -27,18 +29,15 @@ file_logger.setLevel(logging.DEBUG)
 file_logger.setFormatter(fmt)
 logger.addHandler(file_logger)
 logger.debug(f"Logging to {logfile}")
-"""
-
 
-DATA_DIR = "~/Projects/WILDS"
 
 def build_model(name, num_classes, num_domains):
     model = DeepDANN(name, num_classes, num_domains)
     return model
 
-def get_wilds_dataset(name):
-    dataset = get_dataset(dataset=name, root_dir=DATA_DIR, download=False)
-    #logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
+def get_wilds_dataset(name, data_root):
+    dataset = get_dataset(dataset=name, root_dir=data_root, download=False)
+    logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
     return dataset
 
 def get_split(dataset, split_name, transforms=None):
@@ -46,7 +45,6 @@ def get_split(dataset, split_name, transforms=None):
     if split_name == 'test':
         assert False, "Hi! You just tried to load a test split. This line of code is here to prevent you from shooting yourself in the foot. Comment this out to run on test."
     return dataset.get_subset(split_name, transform=transforms)
-
 def train_step(iteration, model, train_loader, loss_class, loss_domain, optimizer, limit_batches=-1):
     model.train()
     all_class_true, all_class_pred, all_metadata, all_domain_pred = [], [], [], []
@@ -78,6 +76,7 @@ def train(train_loader, val_loader, model, n_epochs, get_train_metrics=True, sav
     # train the network
     metrics = []
     for i in range(n_epochs):
+        import pdb; pdb.set_trace()
         all_class_true, all_class_pred, all_metadata, all_domain_pred = train_step(i, model, train_loader, loss_class, loss_domain, optimizer)
         if get_train_metrics:
             train_metrics = dataset.eval(all_class_pred, all_class_true, all_metadata, limit_batches=max_val_batches)
@@ -127,23 +126,31 @@ NUM_CLASSES = {
     "camelyon17": 2,
     "iwildcam": 182
 }
+
 NUM_DOMAINS = {
     "camelyon17": 3,
     "iwildcam": 243
 }
 
+DEFAULT_TRANSFORM = transforms.Compose(
+    [
+        transforms.Resize((224, 224)),
+        transforms.ToTensor()
+    ]
+)
+
 if __name__ == '__main__':
-    print('!!!! Get opts !!!!!\n')
     opts = options.get_opts()
-    print('!!!! Logger !!!!!\n')
-    #logger.info(f"Options:\n{options.prettyprint(opts)}")
-    print('!!!! Get wilds dataset !!!!!\n')
-    dataset = get_wilds_dataset(opts.dataset)
-    print('!!!! Datalodaer !!!!!\n')
-    train_loader, val_loader = get_split(dataset, 'train'), get_split(dataset, 'val')
-    print('!!!! Build model !!!!!\n')
+    logger.info(f"Options:\n{options.prettyprint(opts)}")
+    print(f"Loading dataset {opts.dataset} from {opts.data_root}")
+    dataset = get_wilds_dataset(opts.dataset, opts.data_root)
+    print('Setting up dataloader')
+    train_data, val_data = get_split(dataset, 'train', transforms=DEFAULT_TRANSFORM), get_split(dataset, 'val', transforms=DEFAULT_TRANSFORM)
+    train_loader, val_loader = get_train_loader('standard', train_data, batch_size=opts.batch_size), get_eval_loader('standard', val_data, batch_size=opts.batch_size)
+    print(f'Build model of type {opts.model_name}')
     model = build_model(opts.model_name, NUM_CLASSES[opts.dataset], NUM_DOMAINS[opts.dataset])
 
+    print("Configuring training")
     wandb.init(project='deep-domain-mixup', entity='tchainzzz')
     wandb.config.update(vars(opts))
     metrics = train(train_loader, val_loader, model, opts.n_epochs, get_train_metrics=opts.get_train_metrics, save_every=opts.save_every, max_val_batches=opts.max_val_batches)
diff --git a/options.py b/options.py
index ee1dc35..14e7010 100644
--- a/options.py
+++ b/options.py
@@ -5,7 +5,7 @@ def get_opts():
 
     # data
     psr.add_argument("--dataset", required=True, type=str, choices=['camelyon17', 'iwildcam'])
-
+    psr.add_argument("--data-root", required=True, type=str)
     # training
     psr.add_argument("--model-name", type=str, required=True)
     psr.add_argument("--batch-size", default=16, type=int)
