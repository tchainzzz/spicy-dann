diff --git a/__pycache__/options.cpython-37.pyc b/__pycache__/options.cpython-37.pyc
index 6c8e324..73d1304 100644
Binary files a/__pycache__/options.cpython-37.pyc and b/__pycache__/options.cpython-37.pyc differ
diff --git a/main.py b/main.py
index a9ab555..836451a 100644
--- a/main.py
+++ b/main.py
@@ -10,7 +10,7 @@ import datetime
 import logging
 import os
 import time
-"""
+
 logger = logging.getLogger(__file__)
 logger.setLevel(logging.DEBUG)
 fmt = logging.Formatter("[%(levelname)s] %(asctime)s %(filename)s:%(lineno)d - %(message)s\n")
@@ -27,18 +27,15 @@ file_logger.setLevel(logging.DEBUG)
 file_logger.setFormatter(fmt)
 logger.addHandler(file_logger)
 logger.debug(f"Logging to {logfile}")
-"""
-
 
-DATA_DIR = "~/Projects/WILDS"
 
 def build_model(name, num_classes, num_domains):
     model = DeepDANN(name, num_classes, num_domains)
     return model
 
-def get_wilds_dataset(name):
-    dataset = get_dataset(dataset=name, root_dir=DATA_DIR, download=False)
-    #logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
+def get_wilds_dataset(name, data_root):
+    dataset = get_dataset(dataset=name, root_dir=data_root, download=False)
+    logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
     return dataset
 
 def get_split(dataset, split_name, transforms=None):
@@ -78,6 +75,7 @@ def train(train_loader, val_loader, model, n_epochs, get_train_metrics=True, sav
     # train the network
     metrics = []
     for i in range(n_epochs):
+        import pdb; pdb.set_trace()
         all_class_true, all_class_pred, all_metadata, all_domain_pred = train_step(i, model, train_loader, loss_class, loss_domain, optimizer)
         if get_train_metrics:
             train_metrics = dataset.eval(all_class_pred, all_class_true, all_metadata, limit_batches=max_val_batches)
@@ -133,15 +131,13 @@ NUM_DOMAINS = {
 }
 
 if __name__ == '__main__':
-    print('!!!! Get opts !!!!!\n')
     opts = options.get_opts()
-    print('!!!! Logger !!!!!\n')
-    #logger.info(f"Options:\n{options.prettyprint(opts)}")
-    print('!!!! Get wilds dataset !!!!!\n')
-    dataset = get_wilds_dataset(opts.dataset)
-    print('!!!! Datalodaer !!!!!\n')
+    logger.info(f"Options:\n{options.prettyprint(opts)}")
+    print(f"Loading dataset {opts.dataset} from {opts.data_root}")
+    dataset = get_wilds_dataset(opts.dataset, opts.data_root)
+    print('Setting up dataloader')
     train_loader, val_loader = get_split(dataset, 'train'), get_split(dataset, 'val')
-    print('!!!! Build model !!!!!\n')
+    print(f'Build model of type {opts.model_name}')
     model = build_model(opts.model_name, NUM_CLASSES[opts.dataset], NUM_DOMAINS[opts.dataset])
 
     wandb.init(project='deep-domain-mixup', entity='tchainzzz')
diff --git a/options.py b/options.py
index ee1dc35..14e7010 100644
--- a/options.py
+++ b/options.py
@@ -5,7 +5,7 @@ def get_opts():
 
     # data
     psr.add_argument("--dataset", required=True, type=str, choices=['camelyon17', 'iwildcam'])
-
+    psr.add_argument("--data-root", required=True, type=str)
     # training
     psr.add_argument("--model-name", type=str, required=True)
     psr.add_argument("--batch-size", default=16, type=int)
