diff --git a/__pycache__/options.cpython-37.pyc b/__pycache__/options.cpython-37.pyc
index 6c8e324..73d1304 100644
Binary files a/__pycache__/options.cpython-37.pyc and b/__pycache__/options.cpython-37.pyc differ
diff --git a/main.py b/main.py
index a9ab555..53c6845 100644
--- a/main.py
+++ b/main.py
@@ -1,7 +1,10 @@
 import torch
+import torchvision.transforms as transforms
 from tqdm.auto import tqdm
 import wandb
 from wilds import get_dataset
+from wilds.common.data_loaders import get_train_loader, get_eval_loader
+from wilds.common.grouper import CombinatorialGrouper
 
 from models import DeepDANN
 import options
@@ -10,7 +13,7 @@ import datetime
 import logging
 import os
 import time
-"""
+
 logger = logging.getLogger(__file__)
 logger.setLevel(logging.DEBUG)
 fmt = logging.Formatter("[%(levelname)s] %(asctime)s %(filename)s:%(lineno)d - %(message)s\n")
@@ -27,18 +30,15 @@ file_logger.setLevel(logging.DEBUG)
 file_logger.setFormatter(fmt)
 logger.addHandler(file_logger)
 logger.debug(f"Logging to {logfile}")
-"""
-
 
-DATA_DIR = "~/Projects/WILDS"
 
 def build_model(name, num_classes, num_domains):
     model = DeepDANN(name, num_classes, num_domains)
     return model
 
-def get_wilds_dataset(name):
-    dataset = get_dataset(dataset=name, root_dir=DATA_DIR, download=False)
-    #logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
+def get_wilds_dataset(name, data_root):
+    dataset = get_dataset(dataset=name, root_dir=data_root, download=False)
+    logger.info(f"Loaded dataset {name} with {len(dataset)} examples")
     return dataset
 
 def get_split(dataset, split_name, transforms=None):
@@ -46,8 +46,7 @@ def get_split(dataset, split_name, transforms=None):
     if split_name == 'test':
         assert False, "Hi! You just tried to load a test split. This line of code is here to prevent you from shooting yourself in the foot. Comment this out to run on test."
     return dataset.get_subset(split_name, transform=transforms)
-
-def train_step(iteration, model, train_loader, loss_class, loss_domain, optimizer, limit_batches=-1):
+def train_step(iteration, model, train_loader, grouper, loss_class, loss_domain, optimizer, limit_batches=-1):
     model.train()
     all_class_true, all_class_pred, all_metadata, all_domain_pred = [], [], [], []
     optimizer.zero_grad()
@@ -55,9 +54,12 @@ def train_step(iteration, model, train_loader, loss_class, loss_domain, optimize
         if i == limit_batches:
             logger.warn(f"limit_batches set to {limit_batches}; early exit")
             break
+        raw_metadata = grouper.metadata_to_group(metadata)
+        domain_true = raw_metadata.topk(torch.unique(raw_metadata).numel()).indices # all domain labels are unique and can be deterministically ordered, so use topk
         output = model(x) #TODO: apply mixup, but only to the domain reps?
+        #mixup_criterion(loss_domain, all_domain_pred, all_metadata, output.permutation, output.lam)
         err_class = loss_class(output.logits, y_true)
-        err_domain = loss_domain(output.domain_logits, metadata)
+        err_domain = loss_domain(output.domain_logits, domain_true)
         err = err_class + err_domain
         err.backward()
         optimizer.step()
@@ -67,26 +69,26 @@ def train_step(iteration, model, train_loader, loss_class, loss_domain, optimize
         all_domain_pred += output.domain_logits
     return all_class_true, all_class_pred, all_metadata, all_domain_pred
 
-def train(train_loader, val_loader, model, n_epochs, get_train_metrics=True, save_every=5, max_val_batches=100):
-    # define loss function and optimizer
+
+def train(train_loader, val_loader, model, grouper, n_epochs, get_train_metrics=True, save_every=5, max_val_batches=100):
+    # define loss function and optimizer # TODO: command-line-ify this
     loss_class = torch.nn.NLLLoss()
     loss_domain = torch.nn.NLLLoss()
-    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
+    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
+    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
     #
     for p in model.parameters():
         p.requires_grad = True
     # train the network
     metrics = []
     for i in range(n_epochs):
-        all_class_true, all_class_pred, all_metadata, all_domain_pred = train_step(i, model, train_loader, loss_class, loss_domain, optimizer)
+        import pdb; pdb.set_trace()
+        all_class_true, all_class_pred, all_metadata, all_domain_pred = train_step(i, model, train_loader, grouper, loss_class, loss_domain, optimizer, limit_batches=max_val_batches)
         if get_train_metrics:
             train_metrics = dataset.eval(all_class_pred, all_class_true, all_metadata, limit_batches=max_val_batches)
             train_metrics = metric_eval(all_class_true, all_class_pred, all_metadata, all_domain_pred)
         val_metrics = evaluate(i, model, val_loader, limit_batches=max_val_batches)
-        if i % save_every == 0:
-            torch.save(model.state_dict(), '~/Projects/spicy-dann') # TODO
         metrics.append(val_metrics if not get_train_metrics else (train_metrics, val_metrics))
-        val_metrics = evaluate(i, model, val_loader, limit_batches=max_val_batches)
         if i % save_every == 0:
             torch.save(model.state_dict(), "./models/{run_name}_ep{i}_{human_readable_time}.ckpt")
     return metrics
@@ -106,7 +108,7 @@ def metric_eval(all_class_true, all_class_pred, all_metadata, all_domain_pred):
     metrics = (class_acc, domain_acc)
     return metrics
 
-def evaluate(iteration, model, val_loader, limit_batches=-1):
+def evaluate(iteration, model, val_loader, grouper, limit_batches=-1):
     all_class_true, all_class_pred, all_metadata, all_domain_pred = [], [], [], []
     model.eval()
     with torch.no_grad():
@@ -127,24 +129,40 @@ NUM_CLASSES = {
     "camelyon17": 2,
     "iwildcam": 182
 }
+
 NUM_DOMAINS = {
     "camelyon17": 3,
     "iwildcam": 243
 }
 
+METADATA_KEYS = { # what domain SHIFT are we trying to model?
+    "camelyon17": "hospital",
+    "iwildcam": "location"
+}
+
+DEFAULT_TRANSFORM = transforms.Compose(
+    [
+        transforms.Resize((224, 224)),
+        transforms.ToTensor()
+    ]
+)
+
 if __name__ == '__main__':
-    print('!!!! Get opts !!!!!\n')
     opts = options.get_opts()
-    print('!!!! Logger !!!!!\n')
-    #logger.info(f"Options:\n{options.prettyprint(opts)}")
-    print('!!!! Get wilds dataset !!!!!\n')
-    dataset = get_wilds_dataset(opts.dataset)
-    print('!!!! Datalodaer !!!!!\n')
-    train_loader, val_loader = get_split(dataset, 'train'), get_split(dataset, 'val')
-    print('!!!! Build model !!!!!\n')
+    logger.info(f"Options:\n{options.prettyprint(opts)}")
+    print(f"Loading dataset {opts.dataset} from {opts.data_root}")
+    dataset = get_wilds_dataset(opts.dataset, opts.data_root)
+    print('Setting up dataloader')
+    grouper = CombinatorialGrouper(dataset, [METADATA_KEYS[opts.dataset]])
+    train_data = get_split(dataset, 'train', transforms=DEFAULT_TRANSFORM)
+    val_data = get_split(dataset, 'val', transforms=DEFAULT_TRANSFORM)
+    train_loader = get_train_loader('group', train_data, batch_size=opts.batch_size, grouper=grouper, n_groups_per_batch=min(opts.batch_size, NUM_DOMAINS[opts.dataset]))
+    val_loader = get_eval_loader('group', val_data, batch_size=opts.batch_size, grouper=grouper, n_groups_per_batch=min(opts.batch_size, NUM_DOMAINS[opts.dataset]))
+    print(f'Build model of type {opts.model_name}')
     model = build_model(opts.model_name, NUM_CLASSES[opts.dataset], NUM_DOMAINS[opts.dataset])
 
-    wandb.init(project='deep-domain-mixup', entity='tchainzzz')
+    print("Configuring training")
+    wandb.init(project='deep-domain-mixup', entity='tchainzzz', name=opts.run_name)
     wandb.config.update(vars(opts))
-    metrics = train(train_loader, val_loader, model, opts.n_epochs, get_train_metrics=opts.get_train_metrics, save_every=opts.save_every, max_val_batches=opts.max_val_batches)
+    metrics = train(train_loader, val_loader, model, grouper, opts.n_epochs, get_train_metrics=opts.get_train_metrics, save_every=opts.save_every, max_val_batches=opts.max_val_batches)
     torch.save(model.state_dict(), "./models/{opts.run_name}_final_{human_readable_time}.pth")
diff --git a/models.py b/models.py
index aa277d8..faa86eb 100644
--- a/models.py
+++ b/models.py
@@ -26,7 +26,7 @@ class MixUpDomainClassifier(nn.Module):
             self.layers.append(nn.Linear(in_features, out_features))
         self.layers.append(nn.Linear(fc_size[-1], n_classes))
 
-    def forward(self, X, y):
+    def forward(self, X):
         if self.beta > 0:
             lam = np.random.beta(self.beta, self.beta)
         else:
diff --git a/options.py b/options.py
index ee1dc35..14e7010 100644
--- a/options.py
+++ b/options.py
@@ -5,7 +5,7 @@ def get_opts():
 
     # data
     psr.add_argument("--dataset", required=True, type=str, choices=['camelyon17', 'iwildcam'])
-
+    psr.add_argument("--data-root", required=True, type=str)
     # training
     psr.add_argument("--model-name", type=str, required=True)
     psr.add_argument("--batch-size", default=16, type=int)
